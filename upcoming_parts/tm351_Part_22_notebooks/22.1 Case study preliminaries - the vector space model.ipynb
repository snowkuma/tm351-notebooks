{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "source": [
    "# Case study preliminaries: the vector space model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook you will consider how to implement cosine distance to measure the similarity between text documents. You should spend around one hour on this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we will look at how to represent documents as term frequency vectors in Python, and then how to estimate the similarity of a pair of documents using cosine distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To think about how we will represent a document in Python, let's consider two of the documents that we used in the module materials:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *John likes Mary but Mary likes Peter*\n",
    "\n",
    "and\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *Mary went to the same school as John*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind vector space models of text is that each element in a vector represents the count of words in a corresponding index of terms. So for example, if we had the term index:\n",
    "\n",
    "    ['John', 'Mary', 'Peter', 'as', 'but', 'likes', 'same', 'school', 'the', 'to', 'went']\n",
    "    \n",
    "then we might represent the document *John likes Mary but Mary likes Peter* with the vector:\n",
    "\n",
    "    [1, 2, 1, 0, 1, 2, 0, 0, 0, 0, 0]\n",
    "\n",
    "and *Mary went to the same school as John* as:\n",
    "\n",
    "    [1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *pandas*, a good way to store the vectors would be as a Series, in which the index of the Series is defined using the terms in the documents. We are therefore aiming to convert the sentence *John likes Mary but Mary likes Peter* into a *pandas* Series of the form:\n",
    "\n",
    "|           |   |\n",
    "|:---------:|:-:|\n",
    "|**John**   |1  |\n",
    "|**Mary**   |2  |\n",
    "|**Peter**  |1  |\n",
    "|**as**     |0  |\n",
    "|**but**    |1  |\n",
    "|**likes**  |2  | \n",
    "|**same**   |0  |\n",
    "|**school** |0  | \n",
    "|**the**    |0  |\n",
    "|**to**     |0  |\n",
    "|**went**   |0  |\n",
    "\n",
    "\n",
    "and *Mary went to the same school as John* as:\n",
    "\n",
    "|           |   |\n",
    "|:---------:|:-:|\n",
    "|**John**   |1  |\n",
    "|**Mary**   |1  |\n",
    "|**Peter**  |0  |\n",
    "|**as**     |1  |\n",
    "|**but**    |0  |\n",
    "|**likes**  |0  | \n",
    "|**same**   |1  |\n",
    "|**school** |1  | \n",
    "|**the**    |1  |\n",
    "|**to**     |1  |\n",
    "|**went**   |1  |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will call these vectors *term frequency vectors*, and in the rest of this Notebook we will consider how documents can easily be converted into this form, and how their similarity can then be estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this Notebook we will use the following sentences as our collection of documents:\n",
    "\n",
    "- *John likes Mary but Mary likes Peter*\n",
    "\n",
    "- *Mary went to the same school as John*\n",
    "\n",
    "- *John Smith and John Jones met Mary and John Brown*\n",
    "\n",
    "which we will store as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "documents_ls = ['John likes Mary but Mary likes Peter',\n",
    "                'Mary went to the same school as John',\n",
    "                'John Smith and John Jones met Mary and John Brown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind tokenisation is to create a sequence of the tokens which appear in a document (each *token* is an instance of a *term*). In the first case, our tokenisation mechanism will simply split a document according to the whitespace in that document. So for the document *John likes Mary but Mary likes Peter*, we require a function which will return the list of tokens:\n",
    "\n",
    "    ['John', 'likes', 'Mary', 'but', 'Mary', 'likes', 'Peter']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function will be called `tokenise_document`, which takes a string of text and returns a list of the tokens in the sentence. At the moment, we will use the very simple technique of simply splitting on the whitespace in the document, for which we can use the `split` method which is defined on string objects in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenise_document(docIn_str):\n",
    "    '''Return a list of the tokens in the input string docIn_str'''\n",
    "    return docIn_str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see this on one of the test sentences, try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tokenise_document(documents_ls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the tokenisation function works for the simple algorithm we are implementing at the moment. In later Notebooks we will improve this function, but it is adequate at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of the rest of the work in this Notebook it will be easier to deal with the tokenised sentences. We can create a list of tokenised sentences and store the list in a new variable, `tokenisedDocuments_ls`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tokenisedDocuments_ls = [tokenise_document(doc_txt) for doc_txt in documents_ls]\n",
    "\n",
    "tokenisedDocuments_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the *i*th member of the list `tokenisedDocuments_ls` is the tokenised version of the *i*th member of the list `documents_ls`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a term index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of this Notebook we saw that the index for the term frequency vectors contains the terms in all the documents, with a count of 0 for those terms which do not appear in the document. To build a set of term frequency vectors, we therefore need to be able to identify all the terms which appear in a *collection* of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have defined a function to carry out tokenisation, it is fairly straightforward to find the set of terms which appear in a collection of documents. We can do this by defining a function, `build_term_index`, which takes a collection of tokenised documents and returns a list of the terms which appear in the collection (although we have stored the documents as lists here, we will talk about a collection of documents in case the documents are presented as a set, a Series, or some other container type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def build_term_index(tokenisedDocuments_coll):\n",
    "    '''Return a set of all the terms appearing in the \n",
    "       documents in tokenisedDocuments_coll\n",
    "    '''\n",
    "    allTerms_set = set()  # Store the tokens as a set to remove repetitions\n",
    "    \n",
    "    for tokens_coll in tokenisedDocuments_coll:\n",
    "        allTerms_set = allTerms_set.union(set(tokens_coll))\n",
    "        \n",
    "    return list(allTerms_set)     # Return the members as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for example, to construct the index for the list of sentences stored in `tokenisedDocuments_ls`, we can call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "termIndex_ls = build_term_index(tokenisedDocuments_ls)\n",
    "\n",
    "termIndex_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is the collection of terms that we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building term frequency vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have written a tokenisation function, we can use it to generate term frequency vectors for our documents. To find the term frequencies in a sentence, we can use the `Counter` function in the `collections` library. This returns an object which behaves like a `dict`, containing the number of occurrences of each member of a set. So to illustrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `Counter` function to count the number of times each term appears in a tokenised sentence. So to see how many times each term appears in the first member of our list of tokenised documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "Counter(tokenisedDocuments_ls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this can be converted directly to a `pandas.Series` object, which uses the keys in the dictionary as the index terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(Counter(tokenisedDocuments_ls[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `reindex` method to extend the index to all the terms in the document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(Counter(tokenisedDocuments_ls[0])).reindex(termIndex_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this example shows, *pandas* will create a `Series` object from a `Counter` object by filling in any new terms with `NaN` entries. So the final stage is to replace the `NaN` entries with zero using the `fillna` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(Counter(tokenisedDocuments_ls[0])).reindex(termIndex_ls).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using these techniques we can write a function which takes a tokenised document and an index of terms, and returns a `pd.Series` object representing the term frequency vector for the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def build_tf_vector(tokenisedDocument_ls, termIndex_ls):\n",
    "    '''Return a pandas Series representing the term \n",
    "       frequency vector of the tokenised document \n",
    "       tokenisedDocument_ls, and indexed with termIndex_ls\n",
    "    '''\n",
    "    \n",
    "    return pd.Series(Counter(tokenisedDocument_ls),\n",
    "                     index=termIndex_ls).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "build_tf_vector(tokenisedDocuments_ls[0], \n",
    "                  termIndex_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The function has returned the intended target representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating cosine distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now that we have created the functions needed to represent documents as term vectors, we need to be able to compare them using cosine similarity. Let's start by encoding the first two of the example documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# First document is \"John likes Mary but Mary likes Peter\"\n",
    "termFrequencyVector1_ss = build_tf_vector(tokenisedDocuments_ls[0],\n",
    "                                   termIndex_ls)\n",
    "termFrequencyVector1_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Second document is \"Mary went to the same school as John\n",
    "termFrequencyVector2_ss = build_tf_vector(tokenisedDocuments_ls[1],\n",
    "                                     termIndex_ls)\n",
    "termFrequencyVector2_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in Section 3.3 of Part 22, if ***A*** and ***B*** are term vectors representing two documents, then the cosine similarity is defined as:\n",
    "\n",
    "$$\\cos\\theta = \\frac{\\bf{A} \\cdot \\bf{B}}{\\lvert \\bf{A} \\rvert \\lvert \\bf{B} \\rvert} $$\n",
    "\n",
    "which returns a value between 0 (for documents which share no common tokens) and 1 (for documents which contain exactly the same tokens). This value is often converted into cosine distance by subtracting from 1, so that:\n",
    "\n",
    "$$\\text{distance}({\\bf A}, {\\bf B}) = 1-\\cos\\theta$$\n",
    "\n",
    "so that the more similar the documents are, the smaller the distance between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if the two vectors are:\n",
    "\n",
    "$${\\bf A}=\\langle a_1, a_2, a_3, \\ldots , a_n \\rangle$$\n",
    "\n",
    "and\n",
    "\n",
    "$${\\bf B}=\\langle b_1, b_2, b_3, \\ldots , b_n \\rangle$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\\text{distance}({\\bf A},{\\bf B})=1-\\cos \\theta = 1- \\frac{\\sum_{i=1}^n a_i b_i}{\\sqrt{\\sum_{i=1}^n a_i^2}\\sqrt{\\sum_{i=1}^n b_i^2}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have represented the documents as term frequency vectors, it is reasonably straightforward to calculate the cosine distance between the two documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "1 - sum(termFrequencyVector1_ss * termFrequencyVector2_ss) / (math.sqrt(sum(termFrequencyVector1_ss*termFrequencyVector1_ss)) *\n",
    "                                                              math.sqrt(sum(termFrequencyVector2_ss*termFrequencyVector2_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is also a built-in function to calculate cosine distance in the `scipy.spatial.distance` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "cosine(termFrequencyVector1_ss, termFrequencyVector2_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we would hope, this is the value that we obtained for this task in the module materials, and we will use the built-in function for the rest of the practical work in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it all together: estimating document similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now seen how to represent documents as vectors, and how to use cosine distance to estimate the documents' similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the technique, consider the same cases that we looked at in Exercise 22.1:\n",
    "1. *John likes Mary but Mary likes Peter*\n",
    "2. *Mary went to the same school as John*\n",
    "3. *John Smith and John Jones met Mary and John Brown*\n",
    "\n",
    "You were asked to calculate whether, using cosine distance, document 3 should be considered more similar to document 1 or to document 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We start off by creating strings to encode the three documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "doc1_str = 'John likes Mary but Mary likes Peter'\n",
    "doc2_str = 'Mary went to the same school as John'\n",
    "doc3_str = 'John Smith and John Jones met Mary and John Brown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the `tokenise_document` function to convert the documents to token lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tokenisedDoc1_ls = tokenise_document(doc1_str)\n",
    "tokenisedDoc2_ls = tokenise_document(doc2_str)\n",
    "tokenisedDoc3_ls = tokenise_document(doc3_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, build a term index for the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "termIndex_ls = build_term_index([tokenisedDoc1_ls,\n",
    "                                 tokenisedDoc2_ls,\n",
    "                                 tokenisedDoc3_ls])\n",
    "\n",
    "# Show the terms in the index\n",
    "termIndex_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the term index to construct term vectors for each of the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "termFrequencyVector1_ss = build_tf_vector(tokenisedDoc1_ls, termIndex_ls)\n",
    "termFrequencyVector2_ss = build_tf_vector(tokenisedDoc2_ls, termIndex_ls)\n",
    "termFrequencyVector3_ss = build_tf_vector(tokenisedDoc3_ls, termIndex_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, look at the term vector for the first document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "termFrequencyVector1_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `scipy.spatial.distance.cosine` function, we can now calculate the cosine distance between document 1 and document 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# To calculate the cosine distance between doc 1 and doc 3:\n",
    "cosine(termFrequencyVector1_ss, termFrequencyVector3_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cosine distance between document 2 and document 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# To calculate the cosine distance between doc 2 and doc 3:\n",
    "cosine(termFrequencyVector2_ss, termFrequencyVector3_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine distance between document 1 and document 3 is smaller than that between document 2 and document 3, so we conclude that document 3 is more similar to document 1 than document 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "These results are, as we would have hoped, the same as we calculated manually in the Part 22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "\n",
    "You have now completed this Notebook. Go to Notebook [`22.2 Preliminaries - building the classifier`](22.2 Preliminaries - building the classifier.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
